# ğŸš¨ DIAGNÃ“STICO: TranscriÃ§Ã£o Completamente Errada

## ğŸ”´ Problema Reportado

**VocÃª disse:**
> "Eu falei cerca de 30 segundos e nÃ£o transcreveu NADA que eu falei, ou transcreveu outra coisa completamente diferente"

## ğŸ” PossÃ­veis Causas (em ordem de probabilidade)

### 1. **ğŸ¤ Problema de Captura de Ãudio do Microfone**

**Sintomas:**
- Whisper recebe Ã¡udio, mas o Ã¡udio estÃ¡ corrompido/vazio
- TranscriÃ§Ã£o aleatÃ³ria ou vazia
- Funciona em alguns ambientes mas nÃ£o em outros

**Causas Comuns:**
- âŒ Microfone errado selecionado no navegador
- âŒ Volume do microfone muito baixo
- âŒ Microfone com ruÃ­do excessivo
- âŒ PermissÃµes de microfone bloqueadas
- âŒ Driver de Ã¡udio com problema

**Como Verificar:**
```
1. Abra o Console do navegador (F12)
2. Procure por: "[AudioDebug] chunk"
3. VocÃª deve ver logs como:
   [AudioDebug] chunk 50 downsampledSamples=3333 pcmBytes=6666
   
4. Se nÃ£o aparecer NENHUM log de audio, o microfone NÃƒO estÃ¡ capturando
```

**SoluÃ§Ã£o:**
```powershell
# No Windows, verifique:
1. Settings â†’ System â†’ Sound â†’ Input
2. Fale no microfone e veja se a barra se mexe
3. Ajuste o volume para 80-100%
4. Teste: gravador de voz do Windows
```

---

### 2. **ğŸŒ Problema de Idioma / DetecÃ§Ã£o de Idioma**

**Sintomas:**
- Whisper transcreve, mas em idioma errado
- Palavras aleatÃ³rias que nÃ£o fazem sentido
- Mistura de idiomas

**Causa:**
- VocÃª configurou "I speak: Portuguese" mas Whisper estÃ¡ detectando outro idioma
- Audio muito curto para detectar idioma corretamente
- VAD cortando inÃ­cio/fim da fala

**Logs Esperados (Correto):**
```
ğŸ¤ User xxx spoke in pt: 'OlÃ¡, como vai?'
```

**Logs Problema (Errado):**
```
ğŸ¤ User xxx spoke in en: 'All como vai?'  â† Detectou inglÃªs!
ğŸ¤ User xxx spoke in es: 'Hola como vai?'  â† Detectou espanhol!
```

**SoluÃ§Ã£o TemporÃ¡ria:**
ForÃ§ar o idioma no cÃ³digo (testar se resolve):

```python
# Em ml/asr/whisper_service.py, linha 142
segments, info = self.model.transcribe(
    audio,
    language="pt",  # âœ… FORÃ‡AR portuguÃªs (era: language=language)
    ...
)
```

---

### 3. **âš™ï¸ Modelo Whisper Errado ou Corrompido**

**Sintomas:**
- TranscriÃ§Ã£o sempre errada, independente do que falar
- Palavras sem sentido
- Sempre o mesmo tipo de erro

**Causa:**
- Modelo Whisper "base" pode ser ruim para portuguÃªs
- Modelo nÃ£o baixou corretamente
- Cache corrompido

**SoluÃ§Ã£o:**
```python
# Em backend/main.py ou config
whisper_service.model_size = "medium"  # Usar modelo maior
```

**Limpar cache:**
```powershell
Remove-Item -Recurse -Force ./data/models/whisper/*
# Reiniciar servidor para re-baixar
```

---

### 4. **ğŸ”Š Problema de Sample Rate / Formato de Ãudio**

**Sintomas:**
- Ãudio soa "acelerado" ou "lento" para Whisper
- TranscriÃ§Ã£o parece correta mas de outro idioma
- Palavras distorcidas

**Causa:**
- Frontend envia 16kHz mas backend espera outra coisa
- ConversÃ£o PCM16 com bug

**VerificaÃ§Ã£o:**
No cÃ³digo, temos:
- Frontend: downsamples para 16kHz âœ…
- Backend: espera 16kHz âœ…
- Whisper: requer 16kHz âœ…

**Mas..** se o AudioContext do navegador estÃ¡ em 44.1kHz ou 48kHz e o downsample tem bug, o Ã¡udio fica distorcido.

**Teste:**
```javascript
// No console do navegador (F12):
const context = new AudioContext();
console.log('Sample rate:', context.sampleRate);
// Deve mostrar: 48000 ou 44100
```

---

### 5. **ğŸ›ï¸ VAD Muito Agressivo (Corta a Fala)**

**Sintomas:**
- SÃ³ transcreve palavras soltas
- Frases cortadas no meio
- TranscriÃ§Ã£o incompleta

**Causa:**
- VAD corta inÃ­cio/fim da fala
- Chunks muito pequenos

**JÃ¡ Corrigimos:**
- âœ… Aumentamos intervalo para 500ms
- âœ… Reduzimos threshold VAD (0.6 â†’ 0.4)
- âœ… Filtramos chunks < 300ms

**Se ainda persistir, desabilitar VAD completamente:**

```python
# Em stream_processor.py, linha 160
transcribed_text, detected_lang, _ = await whisper_service.transcribe(
    audio_array,
    language=input_lang if input_lang != 'auto' else None,
    sample_rate=self.input_sample_rate,
    vad_filter=False  # âœ… DESABILITAR VAD para testar
)
```

---

## ğŸ§ª TESTE DEFINITIVO

Vamos criar um teste para identificar exatamente o problema:

### **Teste 1: Verificar se Whisper funciona isoladamente**

```python
# tmp_rovodev_test_whisper_real.py
import numpy as np
import asyncio
from ml.asr.whisper_service import whisper_service

# Carregar modelo
whisper_service.load()

# Criar Ã¡udio de teste (tom puro 440Hz, 1 segundo)
sample_rate = 16000
duration = 1.0
t = np.linspace(0, duration, int(sample_rate * duration))
audio = (np.sin(2 * np.pi * 440 * t) * 0.5).astype(np.float32)

# Transcrever
result = asyncio.run(whisper_service.transcribe(audio, language="pt", sample_rate=16000))
print(f"Resultado: '{result[0]}'")
print(f"Idioma detectado: {result[1]}")

# Deve retornar vazio ou ruÃ­do (pois Ã© sÃ³ um tom)
# Se retornar texto real, Whisper estÃ¡ alucinando!
```

### **Teste 2: Verificar Ã¡udio do navegador**

```javascript
// No console do navegador (F12), cole isso:
navigator.mediaDevices.getUserMedia({ audio: true })
  .then(stream => {
    const context = new AudioContext();
    const source = context.createMediaStreamSource(stream);
    const processor = context.createScriptProcessor(4096, 1, 1);
    
    processor.onaudioprocess = (e) => {
      const data = e.inputBuffer.getChannelData(0);
      const max = Math.max(...data);
      const min = Math.min(...data);
      console.log(`Audio level: ${min.toFixed(3)} to ${max.toFixed(3)}`);
    };
    
    source.connect(processor);
    processor.connect(context.destination);
    
    console.log('âœ… Monitoring audio... speak now!');
    console.log('Sample rate:', context.sampleRate);
  })
  .catch(err => console.error('âŒ Microphone error:', err));

// FALE NO MICROFONE
// VocÃª deve ver logs como:
// Audio level: -0.523 to 0.612  â† ISSO Ã‰ BOM (Ã¡udio detectado)
//
// Se ver:
// Audio level: -0.001 to 0.001  â† RUIM (sem Ã¡udio)
```

---

## ğŸ’¡ SOLUÃ‡ÃƒO RÃPIDA PARA TESTAR AGORA

Vou criar uma versÃ£o que **DESABILITA TODAS as otimizaÃ§Ãµes** e usa configuraÃ§Ã£o mais permissiva:

### **Fix TemporÃ¡rio - Teste de DiagnÃ³stico**

```python
# Em ml/asr/whisper_service.py, linha 142
segments, info = self.model.transcribe(
    audio,
    language="pt",  # âœ… FORÃ‡AR portuguÃªs
    vad_filter=False,  # âœ… DESABILITAR VAD
    beam_size=5,  # âœ… Aumentar qualidade (era 1)
    best_of=5,  # âœ… Aumentar qualidade (era 1)
    temperature=0.0,
    compression_ratio_threshold=2.4,
    log_prob_threshold=-1.0,
    no_speech_threshold=0.8,  # âœ… Mais estrito (era 0.4)
    condition_on_previous_text=True,  # âœ… Ativar contexto
)
```

**E em stream_processor.py, linha 94:**

```python
await asyncio.sleep(1.0)  # âœ… Processar a cada 1 SEGUNDO (era 0.5)
```

**E em stream_processor.py, linha 111:**

```python
if len(combined_chunk) < 32000:  # âœ… MÃ­nimo 1 SEGUNDO (era 0.3s)
    logger.debug(f"â­ï¸ Skipping short audio chunk: {len(combined_chunk)} bytes")
    continue
```

Isso forÃ§a:
- âœ… Idioma portuguÃªs fixo
- âœ… VAD desabilitado
- âœ… Chunks de 1 segundo completo
- âœ… Maior qualidade de transcriÃ§Ã£o

---

## ğŸ“Š O que vocÃª deve ver nos logs:

**ANTES (Bugado):**
```
â­ï¸ Skipping short audio chunk: 4800 bytes
â­ï¸ Skipping short audio chunk: 3200 bytes
â­ï¸ Skipping empty/noise transcription
â­ï¸ Skipping empty/noise transcription
ğŸ¤ User xxx spoke in en: 'random gibberish'  â† ERRADO!
```

**DEPOIS (Correto):**
```
ğŸ¤ User xxx spoke in pt: 'OlÃ¡, tudo bem com vocÃª?'  â† CORRETO!
ğŸŒ Translated to en: 'Hello, how are you?'
âœ… Using cloned voice for user xxx
âœ… User xxx audio processed in 450ms
```

---

## ğŸ¯ PrÃ³ximos Passos

1. **Reinicie o servidor** com as mudanÃ§as
2. **Abra o Console do navegador** (F12)
3. **Entre na sala e fale por 5-10 segundos**
4. **Observe os logs do backend**
5. **Me envie:**
   - Logs do console do navegador
   - Logs do terminal do backend
   - O que vocÃª falou vs o que foi transcrito

Isso vai me dar informaÃ§Ã£o exata para corrigir!

---

## ğŸ”§ Se Nada Funcionar

Ãšltima alternativa: usar outro motor ASR (Google Speech-to-Text, Azure, etc) temporariamente para verificar se Ã© problema do Whisper ou do pipeline de Ã¡udio.

Ou usar Whisper via API externa:
```python
# Teste com OpenAI Whisper API
import openai
openai.api_key = "YOUR_KEY"
result = openai.Audio.transcribe("whisper-1", audio_file)
```

Se funcionar com API externa, o problema Ã©:
- âŒ Whisper local mal configurado
- âŒ Modelo corrompido
- âŒ Driver de Ã¡udio do Windows

Se NÃƒO funcionar nem com API externa, o problema Ã©:
- âŒ Captura de Ã¡udio do navegador
- âŒ Microfone fÃ­sico
- âŒ Formato de Ã¡udio enviado

**Me avise os resultados e vamos resolver juntos!** ğŸš€
